---
title: "Logistic Regression Problem Set"
output: 
  html_document:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# NFL Wins

<a href = '/_Chapter6_ProblemSets/nfl_points_wins.csv'> This csv </a> contains points earned by every team in the NFL and whether or not they won (a 1 is a win and a 0 is a loss) from 1984-2014.^[Much thanks to Ian Kloo for scraping this data.]  We want to build a model to see if we can predict the likelihood a team won its game based solely on the number of points it earned.

## Read and Visualize the Data

```{r, message = F}
library(tidyverse)

# Read the data
nfl <- read.csv('./nfl_points_wins.csv')

# We can just produce a scatter plot, but this is too much...
ggplot(data = nfl, aes(x = points, y = win)) + geom_point()

# We can summarize the data and add color based on numbers of wins at each point level
nfl.temp <- nfl %>% group_by(points, win) %>% summarise(count = n())
# We can then plot each of these points and get a better idea of what's going on
ggplot(data = nfl.temp, aes(x = points, y = win, color = count)) + 
  geom_point() + 
  scale_color_distiller(palette = 'YlOrRd', direction = -1)  # change the color from indecipherable blue
# This is still a bit challenging to see (b/c of many outliers...)
# but you can see roughtly speaking, there are lots of losses with low points and lots of wins with high points
```

## Model

1. Use a logistic regression to model your data.  Interpret your summary results.

```{r}
# conduct the regression using glm
nfl.glm <- glm(win~points, family = binomial, data = nfl)
summary(nfl.glm)

# we see both beta_0 and beta_1 are signficant
```

2. Write out the mathematical model.

``` 
We have beta_0 = -3.4 and beta_1 = .16
```

$$p = \frac{e^{-3.4 + .16x_1}}{1 + e^{-3.4 + .16x_1}}$$

Where $x_1$ are the number of points earned in the game.

3. What does a one point increase indicate in terms of increasing the odds for winning?

```
The odds are:
```

$$ \frac{p}{1-p} = e^{-3.4 + .16x_1} = e^{-3.4} \cdot e^{.16x_1} $$
```
We can then say that an increase of one point increases the odds by 17%
```

4. Plot your model with the data.

```{r}
# Get an input data frame with every point possible from 1 to the max points earned
myLine <- data.frame(points= seq(0, max(nfl$points), by = 1))
# Predict the probability
myLine$prob <- predict(nfl.glm, myLine, type = 'response')

ggplot() + 
  geom_point(data = nfl.temp, aes(x = points, y = win, color = count)) +
  geom_line(data = myLine, aes(x = points, y = prob)) + 
  scale_color_distiller(palette = 'YlOrRd', direction = -1)  # change the color from indecipherable blue

# This is somewhat useful - it fits our intuitive sense that you aren't very likely to win 
# with only a few points and much more likely to win with many points.  
# Of course, it can be hard to see those in between numbers when there are so many results right on top
# of eachother
```

## Check Assumptions

```{r}
# Create a tibble with the deviance residuals and linear predictors
myChecks <- tibble(resid = residuals(nfl.glm), 
                   preds = predict(nfl.glm))

# group into bins of 30
myChecks <- myChecks %>% 
   arrange(preds) %>% 
   mutate(bin = c(rep(1:573, each = 30), rep(574, nrow(nfl) - 573*30))) 

# calculate mean residuals and predictors
myChecks %>% group_by(bin) %>% 
   summarize(
     meanResid = mean(resid),
     meanPred = mean(preds)
   ) %>% 
  # plot the results
  ggplot() + 
  geom_point(aes(x = meanPred, y = meanResid)) + 
  xlab('Fitted Linear Predictor') + 
  ylab('Deviance Residuals')

# The resulting plot looks OK.  There is some overestimation and underestimation at the extremes
# We may see a bit of lack of independence, but hard to say.
# Similarly, we have a higher amount of variance at the middle predictions
# this is to be expected - you're always going to lose if you score 0 points, and almost always going
# to win with sufficiently high points.  The middle level scores are more variable
```

## Assess the Fit

1. Calculate the $R^2$ for the model in the two ways provided in the lesson.

$R^2 = \frac{D_{Null} - D}{D_{Null}}$

```{r}
(nfl.glm$null - nfl.glm$dev) / nfl.glm$null 

# We eexplain about 28% of the variance in success by the number of points won
```

$R^{2}=\frac{1 - exp\left\{ (D-D_{NULL})/N \right\}} {1 - exp\left\{-D_{NULL}/N \right\}}$

```{r}
(1-exp( (nfl.glm$dev - nfl.glm$null)/nrow(nfl))) / (1-exp( (- nfl.glm$null)/nrow(nfl)))

# We explain about 43% of the variance
```

2. Check the goodness of fit using a Hosmer-Lemeshow test.

```{r}
library(ResourceSelection)
p.hat = predict(nfl.glm, type = 'response')
hoslem.test(nfl$win, p.hat)

# In this case we reject our null, as it appears that we do not have a good fit
```

3. Check the goodness of fit using a <a href = 'https://en.wikipedia.org/wiki/Pearson's_chi-squared_test'> Chi Squared test </a>.  You can group by the points earned and calculate your observed values based on those groupings (e.g., if you won 10 times out fo the 20 times you earned five points, your observed value would be .5).  


```{r}
nfl2 <- nfl %>% 
  # group by the points earned
  group_by(points) %>% 
  # calculate number of wins and observations
  summarise(N = n(),
            wins = sum(win)) %>% 
  # get the observed probabilities
  mutate(observed = wins/N) %>% 
  # get the predicted probabilities
  mutate(expected =  predict(nfl.glm, list(points = points), type = 'response')) 

# Calculate the chi squared coefficient
nfl2 %>% mutate(chi = (observed - expected)^2 / expected) %>% summarise(sum(chi))
# calculate the critical value at .95
pchisq(.687, df = 58, lower.tail = F)
# we have a p-value that is effectively 1, saying this is a good model

# We can also plot our results
ggplot() + 
  geom_point(data = nfl2, aes(x = points, y = observed, color = N)) + 
  geom_line(data = nfl2, aes(x = points, y = expected))

```

4. Explain why there may be a difference between the Hosmer Test and the Pearson Chi Squared test.

```
You need to compare the assumptions.  Hosmer test assumes small numbers of observations per grouping (meaning you won't get as accurate of an estimate for your probabilities).  We actually had a large number of observations for most points earned, which gives us a reasonable estimate.  This is where things get tricky.
```

# Regression with Multiple Variables

In the tutorial, we modeled coronary heart disease as a function of age using the data set `SAheart` from the package `bestglm`.  This data set actually has nine possible regressors: systolic blood pressure, tobacco use, etc...

Note, for this question, we are not going to do the full modeling, simply a few exercises to practice logsitic regression with multiple variables.  You are welcome to build the full model, however.

## Assess the Data

Can you use all of this data to conduct a logistic regression?  Why or why not? (Hint, do a visual check with `pairs`)

```{r}
library(bestglm)
pairs(SAheart, upper.panel = NULL)
# it appears that there is some collinearity among the various 
# possible regressors.  Notably, consider that obesity and adiposity
# are essentially the same thing

# We can also look at correlations using corplot
# Note the data has a factor - famhist, so we have to transform it
myData <- SAheart
myData$famhist <- ifelse(myData$famhist == 'Present', 1, 0)
corrplot::corrplot(cor(myData), method = 'number', type = 'lower', diag = F)
```

## Assess Regressors

Conduct a logistic regression to predict coronary heart disease as a function of all the possible regressors (without interactions) (understanding that we may need to actually drop any non-independent regressors).  What regressors are significant at $\alpha = .05$?  Of these, what has the greatest impact on predicting heart disease?

```{r}
# We model this in the same way as we have previously
myModel <- glm(chd ~ ., data = SAheart, family = binomial)
summary(myModel)
# We see that tobacco use, ldl, family history, type a personality, and age are all signfiicant
# Family history has the greatest impact (note the coefficient)

```