---
title: "Support Vector Machines Problem Set"
author: 'Your Name Here'
output: 
  html_document:
    number_sections: T
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This problem set corresponds to the chapter on support vector machines in the course https://doe.dscoe.org. In this problem set, you'll train a support vector regression model on the `airquality` data set from base *R* and a support vector classification model in the `glass` data set from the University of California - Irvine's <a href="https://archive.ics.uci.edu/ml/datasets/glass+identification">Machine Learning Repository</a>. The solutions for this problem set use the following packages; however, you may use whatever packages you prefer.

* `tidyverse`

* `e1071`

* `GGally`

# Support Vector Regression

Remove the NAs from the data, set a random number seed, create a training and a test data set using an 80/20 split, and drop the `Month` and `Day` columns.

```{r message=FALSE, warning=FALSE}
library(tidyverse)

# get the data
data("airquality")

#drop NAs and month and day columns
aq = airquality %>% drop_na() %>% select(-Month, -Day)

# set the seed
set.seed(42)

# create training set
train = sample(1:111, 111*0.8, replace=FALSE)
aq_train = aq %>% slice(train)

# create test set
aq_test = aq %>% slice(-train)
```

## Linear Model

Fit and summarize a linear model with all predictors.

```{r}
aq.lm = lm(Ozone~., data=aq_train)
summary(aq.lm)
```

### Diagnostics

Are any linear model assumptions violated?

```{r}
shapiro.test(residuals(aq.lm))
# residuals are not normally distributed. 
```

### Predictions

Predict Ozone measurements using the linear model and the test data set and calculate the mean squared error (MSE) of the predictions. Recall $MSE = \sum{(predicted - actual)^2}$.

```{r}
lin_preds = predict(aq.lm, newdata = aq_test %>% select(-Ozone))
sum((lin_preds - aq_test$Ozone)^2)
```

## Linear Support Vector Machine

Fit a linear SVM and compare the coefficients with the `lm()` coefficients.

```{r}
library(e1071)

aq.svm.lm = svm(Ozone ~ ., kernel="linear", data=aq_train, scale=F)

coef(aq.svm.lm)
```

### Predictions

Predict Ozone measurements using the SVM linear model and the test data set and calculate the MSE of the predictions. How does it compare to the linear model?

```{r}
svm_lin_preds = predict(aq.svm.lm, newdata = aq_test %>% select(-Ozone))
sum((svm_lin_preds - aq_test$Ozone)^2)
```

## Radial Kernel SVM

Fit an SVM with a radial kernel and calculate the MSE of its predictions. How does this MSE compare to the previous two?

```{r}
aq.svm = svm(Ozone ~ ., data=aq_train)

svm_preds = predict(aq.svm, newdata = aq_test %>% select(-Ozone))
sum((svm_preds - aq_test$Ozone)^2)
```

### Tuned SVM

Tune the radial kernel SVM over a range of gamma and cost values. What were the resulting hyperparameter values?

```{r}
aq.tune = tune.svm(Ozone ~ ., data = aq_train, gamma=seq(0.1, 1, 0.1), cost = seq(1, 100, 10))
print(aq.tune)
```

### Predictions

As before, get predictions for the tuned model and calculate the MSE. How does this MSE compare?

```{r}
tune_preds = predict(aq.tune$best.model, newdata = aq_test %>% select(-Ozone))
sum((tune_preds - aq_test$Ozone)^2)
# MSE is lowest yet!
```

# Support Vector Classification

Download the glass data set from the UC-Irvine repository. This will save you some typing:

```
# glass=read.csv('https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data', 
#                col.names=c("ID", "RI", "Na", "Mg", "Al", "Si", "K", "Ca", "Ba", "Fe", "Type"),
#                header=F, colClasses=c(rep("numeric", 10), "factor"))
```

As indicated on the repository's website, 

>The study of classification of types of glass was motivated by criminological investigation. At the scene of the crime, the glass left can be used as evidence...if it is correctly identified!

The data has 11 columns with the categorical response variable with six levels, `Type`, in the last column. The first column is a sample ID number followed by refractive index and then the weight percent of seven elements in their corresponding oxide. Our task is to see if we can correctly identify the six glass types using a support vector classifier.
  
```{r}
glass=read.csv('https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data', 
               col.names=c("ID", "RI", "Na", "Mg", "Al", "Si", "K", "Ca", "Ba", "Fe", "Type"),
               header=F, colClasses=c(rep("numeric", 10), "factor"))
```

## Train and Test Data Sets

Set a seed and create a training and a test data set using an 80/20 split. Either drop the `ID` column or just don't include it when training your models.

```{r}
# set the seed
set.seed(42)

# create training set
train = sample(1:nrow(glass), nrow(glass)*0.8, replace=FALSE)
glass_train = glass %>% slice(train) %>% select(-ID)

# create test set
glass_test = glass %>% slice(-train) %>% select(-ID)
```


## Plots

There are no missing values, and the data set is considered "well behaved" for a classification problem. Produce one or more scatter plots with your choice of predictors on the x and y axes, and the points color coded by `Type`. Comment on whether it appears this will be a difficult classification task or not.

```{r message=FALSE, warning=FALSE}
color_fn <- function(data, mapping, ...){
  ggplot(data = data, mapping = mapping) + 
    geom_point(aes(color=Type))}

GGally::ggpairs(glass_train[, c(1,2,3,4,5,10)], lower=list(continuous=color_fn), progress=FALSE) + 
  ggtitle("Glass Data Set") +
  theme_bw()

# There seems to be some clustering, but not enough to make me think
# this will be easy.
```

## Linear SVC

Fit a linear support vector machine on the training set using default hyperparameters, use that model to make predictions on the test set, and produce a confusion matrix. How many glass types were misclassified? 

```{r}
glass.lin = svm(Type~., data=glass_train, kernel='linear')

# confusion matrix without a fancy package
lin.tab = table(predict(glass.lin, newdata=glass_test %>% select(-Type)), glass_test$Type)
lin.tab

# the number of misclassifications
nrow(glass_test) - sum(diag(lin.tab))
```

### Accuracy

One measure of the model's accuracy is to divide the number of correct classifications (the diagonal) by the total number of observations. Using this method, what is the model's accuracy? Keep in mind that random guessing would have expected accuracy of 16.7%.

```{r}
sum(diag(lin.tab)) / nrow(glass_test)
```

### Scaling

By default, `svm` scales the predictors. Re-fit the liner SVM model with `scale=FALSE` and produce the confusion matrix. Does scaling seem to help with accuracy?

```{r}
set.seed(42)
glass.lin2 = svm(Type ~ ., data = glass_train, kernel='linear', scale=FALSE)

# confusion matrix
lin.tab2 = table(predict(glass.lin2, newdata=glass_test %>% select(-Type)), glass_test$Type)
lin.tab2

# misclassifications with no scaling
nrow(glass_test) - sum(diag(lin.tab2))
```

## Tuned Linear SVC

Tune a linear SVC over a range of cost, and use the best model to produce and confusion matrix and measure accuracy.

```{r}
# tune the model
lin.tune = tune.svm(Type ~ ., data = glass_train, kernel='linear', cost = c(0.1, 1, 5, 10, 50))

# confusion matrix
lin.tune.tab = table(predict(lin.tune$best.model, newdata=glass_test %>% select(-Type)), glass_test$Type)
lin.tune.tab

# accuracy
sum(diag(lin.tune.tab)) / nrow(glass_test)
```

## Radial SVC

Repeat the above steps, but use a radial kernel with default hyperparameters. What is this model's accuracy?

```{r}
# fit the model
glass.rad = svm(Type ~ ., data = glass_train, kernel='radial')

# confusion matrix
rad.tab = table(predict(glass.rad, newdata=glass_test %>% select(-Type)), glass_test$Type)
rad.tab

# accuracy
sum(diag(rad.tab)) / nrow(glass_test)
```

### Tuned Radial SVC

Tune a radial SVC over a range of cost and gamma, and use the best model to produce and confusion matrix and measure accuracy.

```{r}
# tune the model
glass.rad.tune = tune.svm(Type ~ ., data = glass_train, kernel='radial', 
                          cost = c(0.1, 1, 5, 10, 50),
                          gamma = seq(0.05, 0.15, 0.01))

# confusion matrix
rad.tune.tab = table(predict(glass.rad.tune$best.model, newdata=glass_test %>% select(-Type)), glass_test$Type)
rad.tune.tab

# accuracy
sum(diag(rad.tune.tab)) / nrow(glass_test)
```
