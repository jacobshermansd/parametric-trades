---
footer: "The Research and Analysis Center, Ft. Leavenworth, KS"
output: 
  slidy_presentation:
    font_adjustment: -2
    css: slidy_alt.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
</br>

<center><h1>DoE Methodology Overview</h1></center>
</br>
<center><img src="20190425_TRAC_Logo.png" style="width:300px;height:300px;"/></center>
</br>
<center><h2>02 September 2020</h2></center>
</br>
<center><h3>POC: <a href="mailto:john.f.king1.mil@army.mil">MAJ John King</a></h3></center>

_____
# Outline

* Design of Experiments
  + Design Options
  + EUCOM Scenario Design 
* Regression
  + Simple Linear Regression
  + Assumptions and Diagnostics
  + Multiple Linear Regression
* Machine Learning
  + Regression Tree
  + Random Forest
* Trades Tool Integration

______
# Full Factorial Designs

<div class='left' style='float:left;width:48%'>

* Example Study
  + 3 OMFV systems (main gun, ATGM, and APS)
  + 2 levels each
  + $2^{3} = 8$ AWARS runs

* EUCOM Scenario
  + 15 modernization programs
  + 2 levels each (current vs. future system)
  + $2^{15} = 32,768$ AWARS runs
  
**We need a smaller design!**
</div>

<div class='right' style='float:right;width:48%' align='left'>
<center>**Example Full Factorial Design**</center>

</br>

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)

FFdesign = tibble(
  design_pt = 1:8,
  Main_Gun = rep(c("25mm", "50mm"), 4),
  ATGM = rep(c('TOW', 'TOW', 'JAGM', 'JAGM'), 2),
  APS = rep(c('Smoke', 'Iron Fist'), each=4),
)

FFdesign %>% kbl(align='c') %>% kable_paper(full_width=F)
```

</div>

_____________
# Fractional Factorial Designs

Start with the full factorial design, re-code with -1 and 1, and show all interactions. Then delete lower half of design and remaining colinear columns.

```{r echo=FALSE}
FFdesign2 = tibble(
  DP = 1:8,
  Y_Intercept = rep(1, 8),
  X1 = rep(c(-1, 1), 4),
  X2 = rep(c(-1,-1,1,1), 2),
  X3 = rep(c(-1,1), each=4),
  X1X2 = X1*X2,
  X1X3 = X1*X3,
  X2X3 = X2*X3,
  X1X2X3 = X1*X2*X3
)

FFdesign2 %>% kbl(align='c') %>% kable_paper(full_width=F) %>%
  row_spec(5:8, background='lightgray') %>%
  column_spec(5:8, backgroun='lightgray')
```

The resulting design loses higher order interactions.

```{r echo=FALSE}
r5 = FFdesign2 %>% slice(1:4) %>% select(DP, Y_Intercept, X1, X2, X1X2X3)
colnames(r5) = c('DP', 'Y_Intercept', 'X1', 'X2', 'X3')
r5 %>% kbl(align='c') %>% kable_paper(full_width=F)
```

* The EUCOM design matrix is a Resolution V fractional factorial design which includes:
  + 128 design points
  + Intercept
  + Main effects
  + All 2-way interactions

_____________
# Simple Linear Regression

Purpose: describe a relationship between a predictor variable (x) and a response variable (y).

$$y = \beta_{0} + \beta_{1}x + \epsilon$$

where,

* $\beta_{0}$ is the y-intercept
* $\beta_{1}$ is the slope
* $\epsilon$ is the error in $y$ not explained by $\beta_{0}$ and $\beta_{1}$. 

_____________
## Least Squares Method Graphically

Objective: Minimize the residual sum of squares (RSS).

$RSS = \sum\limits_{i=1}^{n}{(y_{i} - \hat{y}_{i})^2} = \sum\limits_{i=1}^{n}{\hat\epsilon_{i}^{2}}$

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)

df = tibble(
  height = c(66, 54, 50, 74, 59, 53),
  weight = c(141, 128, 123, 160, 136, 139)
)

pts = tibble(x=c(66,66), y=c(141, 146.5))

ggplot(data=df, aes(x=height, y=weight)) +
  geom_point(data=pts, aes(x=x, y=y), size = 8, alpha = 0.25) +
  geom_smooth(data=df, aes(x=height, y=weight), formula=y~x, method="lm", se=FALSE, color='red', size=1.25) +
  annotate("text", x=69, y=143.5, label="Residuals", color='blue', size=8) +
  annotate("text", x=64.5, y=141, label="y[i]", parse=TRUE, size=8) +
  annotate("text", x=64.5, y=146.5, label="hat(y[i])", parse=TRUE, size=8) +
  annotate("text", x=55.5, y=136, label="beta[1]", color='red', parse=TRUE, size=8) +
  annotate("segment", x=50, xend=50, y=123, yend=125.5, color='blue', linetype=2, size=1) +
  annotate("segment", x=53, xend=53, y=139, yend=129.5, color='blue', linetype=2, size=1) +
  annotate("segment", x=54, xend=54, y=128, yend=131.5, color='blue', linetype=2, size=1) +
  annotate("segment", x=59, xend=59, y=136, yend=137.5, color='blue', linetype=2, size=1) +
  annotate("segment", x=66, xend=66, y=141, yend=146.5, color='blue', linetype=2, size=1) +
  annotate("segment", x=74, xend=74, y=160, yend=156.5, color='blue', linetype=2, size=1) +
  geom_point(data=df, aes(x=height, y=weight), size=3) +
  theme_bw()
```

_____________
## Least Squares Method Mathematically

Rewrite expression as:

$\epsilon = y - \beta_{0} + \beta_{1}x$

</br>

If $y= \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix}$ and $X= \begin{pmatrix} 1 & x_1 \\ \vdots & \vdots \\ 1 & x_n \end{pmatrix}$ and $\beta= \begin{pmatrix} \beta_0 \\ \beta_1 \end{pmatrix}$

</br>

Then $\epsilon= \begin{pmatrix} \epsilon_1 \\ \vdots \\ \epsilon_n \end{pmatrix} = \begin{pmatrix} y_1-\beta_0-\beta_1x_1 \\ \vdots \\ y_n-\beta_0-\beta_1x_n \end{pmatrix} = y-X\beta$

</br>

Once we solve for the coefficients, multiplying them by the predictors gives the estimated response, $\hat{y}$.

$X\beta \equiv \hat{y}$

Next step: linear algebra.

_____________
## QR Decomposition

$X\beta \equiv \hat{y}$

$X^TX\beta = X^Ty$

$\beta = (X^TX)^{-1}X^Ty$

Substitute QR for X and solve.

$\beta = ((QR)^T(QR))^{-1}(QR)^Ty$

$\beta = (R^TQ^TQR)^{-1}R^TQ^Ty$

$\beta = (R^TR)^{-1}R^TQ^Ty$

$\beta = R^{-1}R^{-T}R^TQ^Ty$

$\beta = R^{-1}Q^Ty$

In *R*, use `qr(X)` to decompose $X$, and then use `solve.qr()` to calculate the $\beta$ vector.

```{r}
intercept = rep(1, 6)
QR = qr(cbind(intercept, df$height))
(solve.qr(QR, df$weight))
```

For more details, refer to <http://www.seas.ucla.edu/~vandenbe/133A/lectures/ls.pdf>.

_____________
## Linear Regression Assumptions

There are four assumptions fundamental to linear regression:

1. **Linearity:** The relationship between x and the mean of y is linear.
2. **Homoscedasticity:** The variance of residual is the same for any value of x (i.e, constant variance).
3. **Independence:** Independence of the prediction error from every one of the predictor variables.
4. **Normality:** The prediction error is normally distributed.

</br>

If any of these assumptions are violated, then there are three options.

* Change the structure of the model (transformation, use a different known distribution, etc.).
* Accept the violations and use the model anyway.
* Use a non-parametric model (e.g., Random Forest).

_____________
# Multiple Linear Regression

Expand equation to allow for more predictors:

$y = \beta_{0} + \beta_{1}x_{1}+ \beta_{2}x_{2} + ... + \beta_{(p-1)}x_{(p-1)} + \epsilon$

</br>

In matrix form and in terms of $\epsilon$:

$\epsilon= \begin{pmatrix} \epsilon_1 \\ \vdots \\ \epsilon_n \end{pmatrix} = \begin{pmatrix} y_1-\beta_0-\beta_1x_1 \\ \vdots \\ y_n-\beta_0-\beta_1x_n \end{pmatrix} = y-X\beta$

</br>

Note the final equation remains unchanged, so we still solve for the $\beta$ matrix using QR decomposition.

$X\beta \equiv \hat{y}$

________
## Multiple Linear Regression QR Decomposition

One-hot encode the categorical variables to generate X matrix (note: the data below is purely notional).

The true relationship is $y = 10 + 16x_{1}+ 22x_{2} + \epsilon$

```{r echo=FALSE}
set.seed(0)

df = tibble(
  DP = 1:12,
  Program_1 = rep(c("AH-64E", "FARA"), 6),
  Program_2 = rep(c("M2A4", "M2A4", "OMFV", "OMFV"), 3),
  intercept = rep(1, 12),
  program_1_rep = rep(0:1, 6),
  program_2_rep = rep(c(0,0,1,1), 3),
  kills = 10 + 16*program_1_rep + 22*program_2_rep + rnorm(12, sd=5)) # the true relationship

df %>% kbl(align='c') %>% kable_paper(full_width=F) %>%
  add_header_above(c(" "=3, "X Matrix"=3, "Y Matrix" = 1)) %>%
  column_spec(4:6, background='lightblue') %>%           # the X matrix
  column_spec(7, background='lightgreen')                # the y matrix
```

________
## The Resulting Coefficients Using QR Decomposition

```{r echo=FALSE}
X = as.matrix(df[, 4:6])
Y = as.matrix(df[, 7])
QR = qr(X)
solve.qr(QR, Y)
```

### Mathematical Interpretation

$\hat{y} = 12.7 + 14.2x_{1}+ 21.3x_{2}$

**Recall**

$x_1 \begin{cases} & \text{0 if } x_1= AH-64E\\ & \text{1 if } x_1= FARA \end{cases}$

$x_2 \begin{cases} & \text{0 if } x_2= M2A4\\ & \text{1 if } x_2= OMFV \end{cases}$

</br>

<style>
	.demo {
		border:1px solid #000000;
		border-collapse:collapse;
		padding:5px;
	}
	.demo th {
		border:1px solid #000000;
		padding:5px;
		background:#F0F0F0;
	}
	.demo td {
		border:1px solid #000000;
		padding:5px;
	}
</style>
<table class="demo">
	<caption><b>Multiple Linear Regression Interpretation</b></caption>
	<thead>
	<tr>
		<th>If</th>
		<th>Resulting Equation</th>
		<th>Interpretation</th>
	</tr>
	</thead>
	<tbody>
	<tr>
		<td>AH-64E present: x<sub>1</sub>=0<br>M2A4 present: x<sub>2</sub>=0</td>
		<td>&#375; = 12.7 + 14.2(0) + 21.3(0)<br>&#375; = 12.7</td>
		<td>The AH-64E and FARA kill 12.7 threat units.</td>
	</tr>
	<tr>
		<td>FARA present: x<sub>1</sub>=1<br>M2A4 present: x<sub>2</sub>=0</td>
		<td>&#375; = 12.7 + 14.2(1) + 21.3(0)<br>&#375; = 28.9</td>
		<td>Upgrading from the AH-64E to the FARA<br>results in 14.2 additional kills.</td>
	</tr>
	<tr>
		<td>AH-64E present: x<sub>1</sub>=0<br>OMFV present: x<sub>2</sub>=1</td>
		<td>&#375; = 12.7 + 14.2(0) + 21.3(1)<br>&#375; = 34.0</td>
		<td>Upgrading from the M2A4 to the OMFV<br>results in 21.3 additional kills.</td>
	</tr>
	<tr>
		<td>FARA present: x<sub>1</sub>=1<br>OMFV present: x<sub>2</sub>=1</td>
		<td>&#375; = 12.7 + 14.2(1) + 21.3(1)<br>&#375; = 48.2</td>
		<td>Upgrading both systems<br>results in 35.5 additional kills.</td>
	</tr>
	</tbody>
</table>

_________
## The `lm()` Function In *R*

```{r}
df.lm = summary(lm(kills~ program_1_rep + program_2_rep, data=df))
coefficients(df.lm)
```

Because the system of equations is *over-determined* (more equations than unknowns), the `lm()` function also uses QR decomposition.

______
## 2D Graphical Interpretation

The slopes of the lines on the bottom row are equal to the linear model coefficients.

```{r echo=FALSE, message=FALSE, warning=FALSE}
smooth_fn <- function(data, mapping, ...){
  ggplot(data = data, mapping = mapping) + 
    geom_point() + 
    geom_smooth(formula = y~x, method=lm, fill="red", color="red", se=FALSE, ...)
}

GGally::ggpairs(df%>%select(5:7), lower=list(continuous=smooth_fn), progress=FALSE) + 
  ggtitle("Pairs Plot") +
  theme_bw()
```

______
## 3D Graphical Interpretation

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(plotly)

z=c(coefficients(df.lm)[1], sum(coefficients(df.lm)[1:2]), sum(coefficients(df.lm)[c(1,3)]), sum(coefficients(df.lm)[1:3]))
x=c(0,1,0,1)
y=c(0,0,1,1)

plot_ly() %>%
  add_trace(data = df, x=~program_1_rep, y=~program_2_rep, z=~kills,
            type="scatter3d", mode="markers",
            marker=list(color='blue', size=5), showlegend=FALSE) %>%
  add_mesh(x=~x, y=~y, z=~z, showlegend=FALSE, facecolor = c("red", "red"), opacity=0.5) %>%
  add_trace(x=0, y=0, z=10, type="scatter3d", mode="text", text='β<sub>0</sub>', 
            inherit=FALSE, showlegend=FALSE, textfont = list(size = 16)) %>%
  add_trace(x=0.5, y=0, z=14.5, type="scatter3d", mode="text", text='β<sub>1</sub>', 
            inherit=FALSE, showlegend=FALSE, textfont = list(size = 16)) %>%
  add_trace(x=c(0,1), y=c(0,0), z=c(z[1], z[2]), type="scatter3d", mode="lines", 
            line=list(color='black', width=5), showlegend=FALSE) %>%
  add_trace(x=0, y=0.5, z=18, type="scatter3d", mode="text", text='β<sub>2</sub>', 
            inherit=FALSE, showlegend=FALSE, textfont = list(size = 16)) %>%
  add_trace(x=c(0,0), y=c(0,1), z=c(z[1], z[3]), type="scatter3d", mode="lines", 
            line=list(color='black', width=5), showlegend=FALSE) %>%
  layout(
    title = "Best Fit Hyperplane (Without Interaction)",
    scene = list(
      xaxis = list(title = "Program 1"),
      yaxis = list(title = "Program 2"),
      zaxis = list(title = "Kills"),
      camera = list(eye = list(x = -1.25, y = -1.25, z = 1))
    ))
```

__________
## If Interaction Is Present

Add "constructive" interaction to example data.

```{r}
df2 = df %>% mutate(kills = ifelse(program_1_rep==1 & program_2_rep==1, kills+20, kills))
df2.lm = summary(lm(kills ~ program_1_rep + program_2_rep + program_1_rep:program_2_rep, data=df2))
coefficients(df2.lm)
```

* **Without interaction**, upgrading both systems results in 14.2 + 21.3 = 35.5 additional kills.

* **With interaction**, upgrading both systems results in 14.1 + 21.2 + 20.2 = 55.7 additional kills.

```{r echo=FALSE}
z2=c(coefficients(df2.lm)[1], sum(coefficients(df2.lm)[1:2]), sum(coefficients(df2.lm)[c(1,3)]), sum(coefficients(df2.lm)[1:4]))

plot_ly() %>%
  add_trace(data = df2, x=~program_1_rep, y=~program_2_rep, z=~kills,
            type="scatter3d", mode="markers",
            marker=list(color='black', size=5), showlegend=FALSE) %>%
  add_trace(data = df, x=~program_1_rep, y=~program_2_rep, z=~kills,
            type="scatter3d", mode="markers",
            marker=list(color='blue', size=5), showlegend=FALSE) %>%
  add_mesh(x=~x, y=~y, z=~z, showlegend=FALSE, facecolor = c("red"), opacity=0.5) %>%
  add_mesh(x=~x, y=~y, z=~z2, showlegend=FALSE, facecolor = c("black", "red"), opacity=0.5) %>%
  layout(
    title = "Best Fit Hyperplane (With Interaction)",
    scene = list(
      xaxis = list(title = "Program 1"),
      yaxis = list(title = "Program 2"),
      zaxis = list(title = "Kills"),
      camera = list(eye = list(x = 1.25, y = -1.25, z = 0.75))
    ))
```

_________

# Machine Learning

When normality assumptions cannot be met, an alternative is to apply non-parametric machine learning techniques to fit a model. The notional data below represents the number of friendly kills achieved as a function of the presence or absence of eight notional modernization programs. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
set.seed(42)
Full = data.frame(DoE.base::fac.design(nlevels=2, factor.names = paste("P",1:8, sep=''))) %>% mutate_all(~as.numeric(as.character(.))) 
Full = Full - 1
Full = Full %>% mutate(kills = 5 + 5*P1 + 2*P3 + P4 + 10*P6 + 5*P7 + 15*P8 - 2*P3*P7 + 3*P1*P4)
kbl(Full) %>%
  kable_paper() %>%
  add_header_above(c("Notional Data" = 9)) %>%
  scroll_box(height = "200px")
```

For clarity, ease of interpretation, and comparison, no error was introduced to the response variable. The relationship between predictors and the response is:

$$y = 5 + 5P_1 + 0P_2 + 2P_3 + P_4 + 0P_5 + 10P_6 + 5P_7 + 15P_8 - 2(P_3:P_7) + 3(P_1:P_4)$$

***Linear Model Summary***

```{r echo=FALSE, message=FALSE, warning=FALSE}
full.lm = lm(kills~.^2, data=Full)
summary(full.lm)
```

______
## Regression Tree For Binary Predictors

Methodology

* Starting with the full data set, iterate through each predictor, $k_i$, and split the data into two subsets corresponding to $k_i=0$ and $k_i=1$.

* Choose the split that most decreases the total RSS (prediction error).

* Repeat the process for each resulting subset.

* Stop splitting the data using some pre-defined criteria (# observations in a node, tree depth, etc.)

For example, with just one split:

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(rpart)
library(rpart.plot)

full.tree = rpart(kills ~ ., data=Full, cp=0.03)

rpart.plot(full.tree, digits=3, type=4, extra=101)
```

Node Contents:

* Mean kills.

* Number of observations in the node.

* % of observations in the node.

______

### The Full Regression Tree

```{r echo=FALSE}
full.tree = rpart(kills ~ ., data=Full)
rpart.plot(full.tree, digits=3, type=4, extra=101)
```

Notice the first split is on P8, the second set of splits on P6, and the third set on P1. These correspond to the three highest multipliers in the predictor-response equation.

$$y = 5 + 5P_1 + 0P_2 + 2P_3 + P_4 + 0P_5 + 10P_6 + 5P_7 + 15P_8 - 2(P_3:P_7) + 3(P_1:P_4)$$

_______
## Random Forests

Regression trees are relatively weak learners, but they can be improved by growing many trees using **b**ootstrapping and then building an **agg**regated model (referred to as a **bagg**ed model.

### Bootstrap Method

* Repeat 100s or 1000s of times:
  + Randomly sample 2/3 of the data set *with replacement*.
  + Fit a regression tree to the bootstrapped data and save the model.
  
Make predictions using the mean of the individual model predictions.

______
### Example

Need code from my U: drive.

_______
### Return to the 8-Program Data Set

**Random Forest Model Summary**

```{r message=FALSE, warning=FALSE}
library(randomForest)
full.rf = randomForest(kills ~ ., data=Full, importance=TRUE)
full.rf
```

* Model with 500 trees.
* 91% variance explained (due to stopping criteria to avoid overfitting).

**Error ~ Trees**

```{r echo=FALSE}
plot(full.rf)
```

* Observations
  + The error stabilized.
  + A forest of trees has less error than a single tree.
  + Really only need ~100 trees.

_______
### Variable Importance

The rough equivalent of a linear model coefficient is variable importance. It is a measure of how much total reduction in deviance each predictor contributes.

```{r echo=FALSE}
varImpPlot(full.rf)
```

Notice the order from top to bottom is the same as the magnitude of the multipliers in the original equation.

$$y = 5 + 5P_1 + 0P_2 + 2P_3 + P_4 + 0P_5 + 10P_6 + 5P_7 + 15P_8 - 2(P_3:P_7) + 3(P_1:P_4)$$

**Interpretation**: Program 8 contributed the most to kills, followed by Program 6, etc.However, we don't yet know whether it was a *positive* or *negative* contribution.

________
### Partial Dependence

Partial dependence plots are the equivalent of a pairs plot and show whether the predictor variable increased or decreased the response. In this case, each program with a non-flat line contributed positively to the number of kills. 

```{r echo=FALSE}
predictor = iml::Predictor$new(full.rf, data = Full[, 1:8], y = Full$kills)

PDP = iml::FeatureEffects$new(predictor, method='pdp')
PDP$plot() & theme_bw()
```

_________
### Interaction

Tree-based models automatically include interactions, which we can visualize with the following plot.

```{r echo=FALSE, message=FALSE, warning=FALSE}
interact = iml::Interaction$new(predictor)
plot(interact) + theme_bw()
```

**Interpretation**: All predictors have less than 5% of their variation explained by an interaction.

_________
### Interaction Strength

We can also identify what a predictor is interacting with. For example, the plot below shows the strength of interaction with the P1 predictor. As expected, we see the strong interaction with the P4 predictor.

```{r echo=FALSE}
interact1 = iml::Interaction$new(predictor, feature='P1')
plot(interact1) + theme_bw()
```

_________
# Trades Tool Integration


